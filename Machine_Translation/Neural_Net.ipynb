{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJcuZ4Ro+jSgQOAesNTlZ0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smargetic/Natural_Language_Processing/blob/main/Machine_Translation/Neural_Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E38McwRXbyKA"
      },
      "outputs": [],
      "source": [
        "#pytorch\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding_Node(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super(Embedding_Node, self).__init__()\n",
        "\n",
        "    self.W_e = nn.Parameter(torch.randn(vocab_size, embedding_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.W_e[x]"
      ],
      "metadata": {
        "id": "FuiybsIls7m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN_Node(nn.Module):\n",
        "  def __init__(self, hidden_dim):\n",
        "    super(RNN_Node, self).__init__()\n",
        "\n",
        "    #all parameters that go into the rnn node\n",
        "    self.layer_weight = nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
        "    self.hidden_weight = nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
        "    self.bias = nn.Parameter(torch.zeros(hidden_dim))\n",
        "\n",
        "\n",
        "  #how it's connected to the next rnn node\n",
        "  def forward(self, input, prev_hidden):\n",
        "    hidden_node = torch.sigmoid(torch.dot(self.hidden_weight, prev_hidden)+\n",
        "                                     torch.dot(self.layer_weight, input)+\n",
        "                                     self.bias)\n",
        "\n",
        "\n",
        "    return hidden_node\n"
      ],
      "metadata": {
        "id": "odBJfqGfnoLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM_Node(nn.Module):\n",
        "  def __init__(self, hidden_dim):\n",
        "    super(LSTM_Node, self).__init__()\n",
        "\n",
        "    #forget weight / prev layer forget weight / forget bias\n",
        "    self.W_f = nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
        "    self.U_f = nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
        "    self.b_f = nn.Parameter(torch.zeros(hidden_dim))\n",
        "\n",
        "    #input weight / prev layer input weight / input bias\n",
        "    self.W_i = nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
        "    self.U_i = nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
        "    self.b_i = nn.Parameter(torch.zeros(hidden_dim))\n",
        "\n",
        "    #output weight / prev layer output weight / output bis\n",
        "    self.W_o = nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
        "    self.U_o = nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
        "    self.b_o = nn.Parameter(torch.zeros(hidden_dim))\n",
        "\n",
        "    #cell weight / prev layer cell weight / cell bias\n",
        "    self.W_c = nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
        "    self.U_c = nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
        "    self.b_c = nn.Parameter(torch.zeros(hidden_dim))\n",
        "\n",
        "    # #store h_t and c_t\n",
        "    # self.h_t = None\n",
        "    # self.c_t = None\n",
        "\n",
        "  def forward(self, input, prev_hidden, prev_cell):\n",
        "    #forget\n",
        "    f_t = torch.sigmoid(torch.dot(self.W_f, prev_hidden)+\n",
        "                                     torch.dot(self.U_f, input)+\n",
        "                                     self.b_f)\n",
        "    #input\n",
        "    i_t = torch.sigmoid(torch.dot(self.W_i, prev_hidden)+\n",
        "                                     torch.dot(self.U_i, input)+\n",
        "                                     self.b_i)\n",
        "    #output\n",
        "    o_t = torch.sigmoid(torch.dot(self.W_o, prev_hidden)+\n",
        "                                     torch.dot(self.U_o, input)+\n",
        "                                     self.b_o)\n",
        "\n",
        "    #new cell data\n",
        "    nc_t = torch.tanh(torch.dot(self.W_c, prev_hidden)+\n",
        "                                     torch.dot(self.U_c, input)+\n",
        "                                     self.b_c)\n",
        "\n",
        "    #cell\n",
        "    c_t = f_t*prev_cell + i_t*nc_t\n",
        "\n",
        "    #hidden\n",
        "    h_t = o_t*torch.tanh(c_t)\n",
        "\n",
        "    # #store vals\n",
        "    # self.h_t = h_t\n",
        "    # self.c_t = c_t\n",
        "\n",
        "    return h_t, c_t\n",
        "\n",
        "\n",
        "  # def get_hidden(self):\n",
        "  #   return self.h_t\n",
        "\n",
        "  # def get_cell(self):\n",
        "  #   return self.c_t\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "xuBYxuXSsus3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for this, we basically said that what should be in the __init__ function should be any parameters that could be learned\n",
        "\n",
        "#ok, so for node elements, we'll say that in the __init__ funciton we should have the parameters whos weight should be learned\n",
        "#but for nn layers, well have things that define the layers/ hyperparameters\n",
        "\n",
        "#__init__ layer = anything that needs to be consistently referenced in forward pass (especially parameters that need to be updated)"
      ],
      "metadata": {
        "id": "ZubrtWaX4r7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderNN(nn.Module):\n",
        "  def __init__(self, node_type, n_layers, hidden_dim, vocab_size):\n",
        "    super(EncoderNN, self).__init__()\n",
        "\n",
        "    #model parameters\n",
        "    self.n_layers = n_layers\n",
        "    self.node_type = node_type\n",
        "\n",
        "    #define embedding node\n",
        "    self.embedding_node = Embedding_Node(vocab_size, hidden_dim) # I guess this is one node?\n",
        "\n",
        "    #create layer of nodes based on type\n",
        "    if(node_type==\"rnn\"):\n",
        "      self.nodes = nn.ModuleList([RNN_Node(hidden_dim) for i in range(0,n_layers)])\n",
        "    else:\n",
        "      self.nodes = nn.ModuleList([LSTM_Node(hidden_dim) for i in range(0,n_layers)])\n",
        "\n",
        "    #store initial hidden states\n",
        "    self.hidden = torch.randn(n_layers, hidden_dim)\n",
        "    self.cell = torch.randn(n_layers, hidden_dim) #only really applicable for lstm\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, inputs):\n",
        "\n",
        "    #get initial ndoes\n",
        "    node_list = self.nodes\n",
        "\n",
        "    #take initial hidden states - 1 for each layer\n",
        "    hidden_list = self.hidden\n",
        "    cell_list = self.cell\n",
        "\n",
        "    for word in inputs:\n",
        "      #input is initial embedding\n",
        "      input = self.embedding_node.forward(word)\n",
        "\n",
        "      for i in range(0,self.n_layers):\n",
        "        #if not first word, then previous output\n",
        "        if i!=0:\n",
        "          input = hidden_list[i-1]\n",
        "\n",
        "        #forward nodes\n",
        "        if(self.node_type==\"rnn\"):\n",
        "          hidden_list[i] = node_list[i].forward(input, hidden_list[i])\n",
        "        else:\n",
        "          self.hidden_list[i], cell_list[i] = node_list[i].forward(input, hidden_list[i], cell_list[i])\n",
        "\n",
        "    outputs = hidden_list if self.node_type==\"rnn\" else (hidden_list, cell_list)\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "KUrdwxzVmO4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderNN(nn.Module):\n",
        "  def __init__(self, node_type, n_layers, hidden_dim, vocab_size):\n",
        "    super(DecoderNN, self).__init__()\n",
        "\n",
        "    #model parameters\n",
        "    self.n_layers = n_layers\n",
        "    self.node_type = node_type\n",
        "\n",
        "    #define embedding node\n",
        "    self.embedding_node = Embedding_Node(vocab_size, hidden_dim) # I guess this is one node?\n",
        "\n",
        "    #create layer of nodes based on type\n",
        "    if(node_type==\"rnn\"):\n",
        "      self.nodes = nn.ModuleList([RNN_Node(hidden_dim) for i in range(0,n_layers)])\n",
        "    else:\n",
        "      self.nodes = nn.ModuleList([LSTM_Node(hidden_dim) for i in range(0,n_layers)])\n",
        "\n",
        "    #store initial hidden states\n",
        "    self.hidden = torch.randn(n_layers, hidden_dim)\n",
        "    self.cell = torch.randn(n_layers, hidden_dim) #only really applicable for lstm\n",
        "\n",
        "    #output weights\n",
        "    self.W_out = nn.Parameter(torch.randn(vocab_size, hidden_dim))\n",
        "\n",
        "  def forward(self, *args):\n",
        "\n",
        "    ### pseudo code\n",
        "\n",
        "    #while the output is not the eof token\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gKiXWgCvfAuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder_args, decoder_args):\n",
        "    super(Seq2Seq, self).__init__()\n",
        "\n",
        "    # node_type, n_layers, hidden_dim, vocab_size\n",
        "\n",
        "\n",
        "    #model structure\n",
        "    self.encoder = EncoderNN(encoder_args)\n",
        "    self.decoder = DecoderNN(decoder_args)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "\n",
        "    if (self.encoder.node_type == \"rnn\"):\n",
        "      hidden_list = self.encoder.forward(inputs)\n",
        "    else:\n",
        "      hidden_list, cell_list = self.encoder.forward(inputs)\n",
        "\n",
        "    if(self.decoder.node_type == \"rnn\"):\n",
        "      outputs = self.decoder.forward(hidden_list)\n",
        "    else:\n",
        "      hidden_list, cell_list = self.decoder.forward(hidden_list, cell_list)\n",
        "\n",
        "    return outputs\n"
      ],
      "metadata": {
        "id": "RQS3dWtMfNjg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}