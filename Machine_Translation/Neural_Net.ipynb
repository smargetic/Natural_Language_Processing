{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smargetic/Natural_Language_Processing/blob/main/Machine_Translation/Neural_Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-eYx0G4p02W",
        "outputId": "c855cee6-968d-42ca-fbb5-657e024de3c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (6.5.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert) (4.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (0.4)\n",
            "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (3.1.4)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (5.7.2)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert) (0.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (2.1.5)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (0.10.0)\n",
            "Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (5.10.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nbconvert) (24.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (1.5.1)\n",
            "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (2.18.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (1.3.0)\n",
            "Requirement already satisfied: traitlets>=5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (5.7.1)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert) (4.3.6)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from nbclient>=0.5.0->nbconvert) (6.1.12)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert) (2.20.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert) (4.23.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert) (2.6)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert) (0.5.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert) (0.20.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (24.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (2.8.2)\n",
            "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (6.3.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install nbconvert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E38McwRXbyKA",
        "outputId": "ac45f814-0eb8-48e4-f254-4eebba9404d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#data storage - not sure if important\n",
        "import pandas as pd\n",
        "\n",
        "#make sure to show all values in pandas\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "#pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#visualize\n",
        "from IPython.display import display\n",
        "\n",
        "#help import other functions\n",
        "import os\n",
        "\n",
        "#mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJRgbpQDJyVT",
        "outputId": "33cbbfc7-192e-40a4-de7e-13c261294711"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Files currently present:\n",
            "['Neural_Net.ipynb', 'sentences_base.csv', 'ita_sentences_CC0.tsv', 'eng_sentences_detailed.tsv', 'eng_sentences.tsv', 'ita_sentences_detailed.tsv', 'eng_sentences_CC0.tsv', 'ita_sentences.tsv', 'Sentence pairs in English-Italian - 2024-07-26.tsv', 'Helper_Functions.ipynb', 'Model_Train_Predict.ipynb', '__pycache__', 'Data_Intake_and_Preprocessing.ipynb', 'intake_preprocess_utils.ipynb', 'intake_preprocess_utils.py']\n",
            "[NbConvertApp] Converting notebook intake_preprocess_utils.ipynb to script\n",
            "[NbConvertApp] Writing 11518 bytes to intake_preprocess_utils.txt\n",
            "\n",
            "\n",
            "New files in current directory:\n",
            "['Neural_Net.ipynb', 'sentences_base.csv', 'ita_sentences_CC0.tsv', 'eng_sentences_detailed.tsv', 'eng_sentences.tsv', 'ita_sentences_detailed.tsv', 'eng_sentences_CC0.tsv', 'ita_sentences.tsv', 'Sentence pairs in English-Italian - 2024-07-26.tsv', 'Helper_Functions.ipynb', 'Model_Train_Predict.ipynb', '__pycache__', 'Data_Intake_and_Preprocessing.ipynb', 'intake_preprocess_utils.ipynb', 'intake_preprocess_utils.py']\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "##to obtain functions from another .ipynb file\n",
        "\n",
        "#go to current directory\n",
        "os.chdir('/content/drive/My Drive/Coding Projects/Machine Translation')\n",
        "\n",
        "#List the files in the current directory\n",
        "print('\\nFiles currently present:')\n",
        "print(os.listdir())\n",
        "\n",
        "#Convert the notebook to a Python script\n",
        "!jupyter nbconvert --to script intake_preprocess_utils.ipynb\n",
        "os.rename('intake_preprocess_utils.txt', 'intake_preprocess_utils.py')\n",
        "\n",
        "print('\\n\\nNew files in current directory:')\n",
        "print(os.listdir())\n",
        "print('\\n\\n')\n",
        "\n",
        "#import functions\n",
        "from intake_preprocess_utils import get_file_and_disp, get_sample, dump_sample, data_preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOifKYsyxoIs"
      },
      "outputs": [],
      "source": [
        "# #english to italian translation - just to test\n",
        "# df_eng_it = get_file_and_disp(fileName=\"Sentence pairs in English-Italian - 2024-07-26.tsv.zip\" ,sep='\\t', stringName=\"English to Italian\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test data set\n",
        "df = pd.DataFrame({\n",
        "                  0:[1,2,3],\n",
        "                  1:[\"hello world\", \"how are you\", \"where are we going\"],\n",
        "                  2:[1,2,3],\n",
        "                  3: [\"ciao mondo\", \"come stai\", \"dove stiamo andando\"]})\n",
        "\n",
        "display(df)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "mRoEVDsAWxDV",
        "outputId": "e68aded6-0b01-4add-a559-3d0c3de1c620"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   0                   1  2                    3\n",
              "0  1         hello world  1           ciao mondo\n",
              "1  2         how are you  2            come stai\n",
              "2  3  where are we going  3  dove stiamo andando"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ad988860-1b75-40a5-965b-81ed780dae5a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>hello world</td>\n",
              "      <td>1</td>\n",
              "      <td>ciao mondo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>how are you</td>\n",
              "      <td>2</td>\n",
              "      <td>come stai</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>where are we going</td>\n",
              "      <td>3</td>\n",
              "      <td>dove stiamo andando</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ad988860-1b75-40a5-965b-81ed780dae5a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ad988860-1b75-40a5-965b-81ed780dae5a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ad988860-1b75-40a5-965b-81ed780dae5a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a1cd7c35-c8f3-476a-a098-d97da78a8934\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a1cd7c35-c8f3-476a-a098-d97da78a8934')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a1cd7c35-c8f3-476a-a098-d97da78a8934 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_704bac43-c3c9-4dba-b829-7837e9109782\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_704bac43-c3c9-4dba-b829-7837e9109782 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 1,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"hello world\",\n          \"how are you\",\n          \"where are we going\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 2,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 3,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"ciao mondo\",\n          \"come stai\",\n          \"dove stiamo andando\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#go through preprocessing\n",
        "data_dict= data_preprocessing(df.copy(), pytorchB=True, tensorflowB=False, store=False, sample=False, download=False, sing=False)\n",
        "for key in data_dict.keys():\n",
        "  print('\\n'+key+':')\n",
        "  display(data_dict[key])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QmDxJKD6dQ2j",
        "outputId": "f7bbf64d-b83d-4fb0-aa9e-8b832c09b6d1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nulls in English: 0\n",
            "Nulls in Italian: 0\n",
            "\n",
            "English/Italian Translations:\n",
            "\tEnglish vocabulary size: 11\n",
            "\tItalian vocabulary size: 10\n",
            "\n",
            "Done with Pytorch.\n",
            "\n",
            "English-Italian Dataframe:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   eng_id        eng_sentence  it_id          it_sentence  \\\n",
              "0       1         hello world      1           ciao mondo   \n",
              "1       2         how are you      2            come stai   \n",
              "2       3  where are we going      3  dove stiamo andando   \n",
              "\n",
              "                              eng_tokens  \\\n",
              "0           [<sos>, hello, world, <eos>]   \n",
              "1          [<sos>, how, are, you, <eos>]   \n",
              "2  [<sos>, where, are, we, going, <eos>]   \n",
              "\n",
              "                               it_tokens      eng_tokens_enc    it_tokens_enc  \\\n",
              "0            [<sos>, ciao, mondo, <eos>]        [1, 5, 9, 2]     [1, 4, 7, 2]   \n",
              "1             [<sos>, come, stai, <eos>]    [1, 6, 3, 10, 2]     [1, 5, 8, 2]   \n",
              "2  [<sos>, dove, stiamo, andando, <eos>]  [1, 8, 3, 7, 4, 2]  [1, 6, 9, 3, 2]   \n",
              "\n",
              "                                                      eng_tokens_enc_p  \\\n",
              "0   [tensor(1), tensor(5), tensor(9), tensor(2), tensor(0), tensor(0)]   \n",
              "1  [tensor(1), tensor(6), tensor(3), tensor(10), tensor(2), tensor(0)]   \n",
              "2   [tensor(1), tensor(8), tensor(3), tensor(7), tensor(4), tensor(2)]   \n",
              "\n",
              "                                           it_tokens_enc_p  \n",
              "0  [tensor(1), tensor(4), tensor(7), tensor(2), tensor(0)]  \n",
              "1  [tensor(1), tensor(5), tensor(8), tensor(2), tensor(0)]  \n",
              "2  [tensor(1), tensor(6), tensor(9), tensor(3), tensor(2)]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-73f2f257-5bfa-4865-85eb-9cc2b1f30287\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eng_id</th>\n",
              "      <th>eng_sentence</th>\n",
              "      <th>it_id</th>\n",
              "      <th>it_sentence</th>\n",
              "      <th>eng_tokens</th>\n",
              "      <th>it_tokens</th>\n",
              "      <th>eng_tokens_enc</th>\n",
              "      <th>it_tokens_enc</th>\n",
              "      <th>eng_tokens_enc_p</th>\n",
              "      <th>it_tokens_enc_p</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>hello world</td>\n",
              "      <td>1</td>\n",
              "      <td>ciao mondo</td>\n",
              "      <td>[&lt;sos&gt;, hello, world, &lt;eos&gt;]</td>\n",
              "      <td>[&lt;sos&gt;, ciao, mondo, &lt;eos&gt;]</td>\n",
              "      <td>[1, 5, 9, 2]</td>\n",
              "      <td>[1, 4, 7, 2]</td>\n",
              "      <td>[tensor(1), tensor(5), tensor(9), tensor(2), tensor(0), tensor(0)]</td>\n",
              "      <td>[tensor(1), tensor(4), tensor(7), tensor(2), tensor(0)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>how are you</td>\n",
              "      <td>2</td>\n",
              "      <td>come stai</td>\n",
              "      <td>[&lt;sos&gt;, how, are, you, &lt;eos&gt;]</td>\n",
              "      <td>[&lt;sos&gt;, come, stai, &lt;eos&gt;]</td>\n",
              "      <td>[1, 6, 3, 10, 2]</td>\n",
              "      <td>[1, 5, 8, 2]</td>\n",
              "      <td>[tensor(1), tensor(6), tensor(3), tensor(10), tensor(2), tensor(0)]</td>\n",
              "      <td>[tensor(1), tensor(5), tensor(8), tensor(2), tensor(0)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>where are we going</td>\n",
              "      <td>3</td>\n",
              "      <td>dove stiamo andando</td>\n",
              "      <td>[&lt;sos&gt;, where, are, we, going, &lt;eos&gt;]</td>\n",
              "      <td>[&lt;sos&gt;, dove, stiamo, andando, &lt;eos&gt;]</td>\n",
              "      <td>[1, 8, 3, 7, 4, 2]</td>\n",
              "      <td>[1, 6, 9, 3, 2]</td>\n",
              "      <td>[tensor(1), tensor(8), tensor(3), tensor(7), tensor(4), tensor(2)]</td>\n",
              "      <td>[tensor(1), tensor(6), tensor(9), tensor(3), tensor(2)]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-73f2f257-5bfa-4865-85eb-9cc2b1f30287')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-73f2f257-5bfa-4865-85eb-9cc2b1f30287 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-73f2f257-5bfa-4865-85eb-9cc2b1f30287');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c54b09e1-441f-4b78-92ec-4f5df7e77ca5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c54b09e1-441f-4b78-92ec-4f5df7e77ca5')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c54b09e1-441f-4b78-92ec-4f5df7e77ca5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"  display(data_dict[key])\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"eng_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"hello world\",\n          \"how are you\",\n          \"where are we going\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"it_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"it_sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"ciao mondo\",\n          \"come stai\",\n          \"dove stiamo andando\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_tokens\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"it_tokens\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_tokens_enc\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"it_tokens_enc\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_tokens_enc_p\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"tensor([1, 5, 9, 2, 0, 0])\",\n          \"tensor([ 1,  6,  3, 10,  2,  0])\",\n          \"tensor([1, 8, 3, 7, 4, 2])\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"it_tokens_enc_p\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"tensor([1, 4, 7, 2, 0])\",\n          \"tensor([1, 5, 8, 2, 0])\",\n          \"tensor([1, 6, 9, 3, 2])\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "English-Italian SING Dataframe:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "English-Italian Pytorch Dataloader:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7daebc58e680>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "English-Italian TensorFlow Dataloader:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "English-Italian Pytorch SING Dataloader:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "English-Italian TensorFlow SING Dataloader:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "English Vocabulary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'<sos>': 1,\n",
              " '<eos>': 2,\n",
              " 'are': 3,\n",
              " 'going': 4,\n",
              " 'hello': 5,\n",
              " 'how': 6,\n",
              " 'we': 7,\n",
              " 'where': 8,\n",
              " 'world': 9,\n",
              " 'you': 10,\n",
              " '<pad>': 0}"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Italian Vocabulary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'<sos>': 1,\n",
              " '<eos>': 2,\n",
              " 'andando': 3,\n",
              " 'ciao': 4,\n",
              " 'come': 5,\n",
              " 'dove': 6,\n",
              " 'mondo': 7,\n",
              " 'stai': 8,\n",
              " 'stiamo': 9,\n",
              " '<pad>': 0}"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "English Vocabulary SING:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Italian Vocabulary SING:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FuiybsIls7m1"
      },
      "outputs": [],
      "source": [
        "#forward input: (batch size (aka. # sentences), # words in sentence (including padding))\n",
        "#forward output: (batch size (aka. # sentences), # words in sentence (including padding), embedding dim)\n",
        "class Embedding_Node(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super(Embedding_Node, self).__init__()\n",
        "\n",
        "    self.W_e = nn.Parameter(torch.randn(vocab_size, embedding_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.W_e[x]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### NOT SURE WHAT THIS IS ---> NEED TO FIX\n",
        "\n",
        "for input_batch, target_batch in data_dict['English-Italian Pytorch Dataloader']:\n",
        "  print('\\nbatch:')\n",
        "  display(input_batch)\n",
        "  display(target_batch)\n",
        "\n",
        "print('\\n\\nbatch:')\n",
        "display(input_batch)\n",
        "display(target_batch)\n",
        "\n",
        "#get input and output\n",
        "input, output = input_batch, target_batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "tjcCtOuDuH_t",
        "outputId": "323df93d-d925-469b-c071-4f5d017b3205"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "batch:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tensor([[ 1,  5,  9,  2,  0,  0],\n",
              "        [ 1,  6,  3, 10,  2,  0],\n",
              "        [ 1,  8,  3,  7,  4,  2]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tensor([[1, 4, 7, 2, 0],\n",
              "        [1, 5, 8, 2, 0],\n",
              "        [1, 6, 9, 3, 2]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "batch:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tensor([[ 1,  5,  9,  2,  0,  0],\n",
              "        [ 1,  6,  3, 10,  2,  0],\n",
              "        [ 1,  8,  3,  7,  4,  2]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tensor([[1, 4, 7, 2, 0],\n",
              "        [1, 5, 8, 2, 0],\n",
              "        [1, 6, 9, 3, 2]])"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ctGekK2XXec4"
      },
      "outputs": [],
      "source": [
        "class Attention_Node(nn.Module):\n",
        "  def __init__(self,hidden_dim):\n",
        "    self.hidden_dim = hidden_dim\n",
        "\n",
        "    #key, query, value\n",
        "    self.W_key = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.W_query = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.W_value = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "  def forward(self, key_hidden, query_hidden, masked=False):\n",
        "    #key, query, value\n",
        "    key = self.W_key(key_hidden)\n",
        "    query = self.W_query(query_hidden)\n",
        "    value = self.W_value(key_hidden)\n",
        "\n",
        "    #get attention scores\n",
        "    attention_score = torch.matmul(query.T, key)\n",
        "\n",
        "    #masked - make upper triangle of values negative inf\n",
        "    if(masked):\n",
        "      mask = torch.triu(torch.ones_like(attention_score), diagonal=0).bool()\n",
        "      attention_score[~mask] = float('-inf')\n",
        "\n",
        "    #softmax\n",
        "    attention_dist = F.softmax(attention_score, dim=0)\n",
        "\n",
        "    #attention output - weighted sum\n",
        "    attention_output = torch.dot(attention_dist, value)\n",
        "\n",
        "    return attention_output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# att_node = Attention_Node(embedding_dim)"
      ],
      "metadata": {
        "id": "RFJnOWSM4CxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odBJfqGfnoLZ"
      },
      "outputs": [],
      "source": [
        "class RNN_Node(nn.Module):\n",
        "  def __init__(self, hidden_dim, embed_dim):\n",
        "    super(RNN_Node, self).__init__()\n",
        "\n",
        "    #all parameters that go into the rnn node\n",
        "    self.layer_weight = nn.Parameter(torch.randn(hidden_dim, embed_dim))\n",
        "    self.hidden_weight = nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
        "    self.bias = nn.Parameter(torch.zeros(hidden_dim))\n",
        "\n",
        "    #store dimensions\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.embed_dim = embed_dim\n",
        "\n",
        "  #forward pass is how its connected to the next rnn node (regardless of layer or column)\n",
        "  def forward(self, input, prev_hidden):\n",
        "\n",
        "    #broadcast bias\n",
        "    bias = self.bias.expand(input.shape[0], self.hidden_dim)\n",
        "\n",
        "    #get hidden node\n",
        "    hidden_node = torch.sigmoid(torch.matmul(self.hidden_weight, prev_hidden.T).T+\n",
        "                                     torch.matmul(self.layer_weight, input.T).T+\n",
        "                                     bias) #could also do tanh instead of sigmoid\n",
        "\n",
        "    return hidden_node\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f8vTC4ea4iUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuBYxuXSsus3"
      },
      "outputs": [],
      "source": [
        "class LSTM_Node(nn.Module):\n",
        "  def __init__(self, hidden_dim, embed_dim):\n",
        "    super(LSTM_Node, self).__init__()\n",
        "\n",
        "    #forget weight / prev layer forget weight / forget bias\n",
        "    self.W_f = nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
        "    self.U_f = nn.Parameter(torch.randn(hidden_dim, embed_dim))\n",
        "    self.b_f = nn.Parameter(torch.zeros(hidden_dim))\n",
        "\n",
        "    #input weight / prev layer input weight / input bias\n",
        "    self.W_i = nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
        "    self.U_i = nn.Parameter(torch.randn(hidden_dim, embed_dim))\n",
        "    self.b_i = nn.Parameter(torch.zeros(hidden_dim))\n",
        "\n",
        "    #output weight / prev layer output weight / output bis\n",
        "    self.W_o = nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
        "    self.U_o = nn.Parameter(torch.randn(hidden_dim, embed_dim))\n",
        "    self.b_o = nn.Parameter(torch.zeros(hidden_dim))\n",
        "\n",
        "    #cell weight / prev layer cell weight / cell bias\n",
        "    self.W_c = nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
        "    self.U_c = nn.Parameter(torch.randn(hidden_dim, embed_dim))\n",
        "    self.b_c = nn.Parameter(torch.zeros(hidden_dim))\n",
        "\n",
        "    #store dimensions\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.embed_dim = embed_dim\n",
        "\n",
        "\n",
        "  #forward pass is how its connected to the next lstm node (regardless of layer or column)\n",
        "  def forward(self, input, prev_hidden, prev_cell):\n",
        "\n",
        "\n",
        "    #broadcast bias\n",
        "    b_f = self.b_f.expand(input.shape[0], self.hidden_dim)\n",
        "    b_i = self.b_i.expand(input.shape[0], self.hidden_dim)\n",
        "    b_o = self.b_o.expand(input.shape[0], self.hidden_dim)\n",
        "    b_c = self.b_c.expand(input.shape[0], self.hidden_dim)\n",
        "\n",
        "\n",
        "    #forget\n",
        "    f_t = torch.sigmoid(torch.matmul(self.W_f, prev_hidden.T).T+\n",
        "                                     torch.matmul(self.U_f, input.T).T+\n",
        "                                     b_f)\n",
        "    #input\n",
        "    i_t = torch.sigmoid(torch.matmul(self.W_i, prev_hidden.T).T+\n",
        "                                     torch.matmul(self.U_i, input.T).T+\n",
        "                                     b_i)\n",
        "    #output\n",
        "    o_t = torch.sigmoid(torch.matmul(self.W_o, prev_hidden.T).T+\n",
        "                                     torch.matmul(self.U_o, input.T).T+\n",
        "                                     b_o)\n",
        "\n",
        "    #new cell data\n",
        "    nc_t = torch.tanh(torch.matmul(self.W_c, prev_hidden.T).T+\n",
        "                                     torch.matmul(self.U_c, input.T).T+\n",
        "                                     b_c)\n",
        "\n",
        "    #cell\n",
        "    c_t = f_t*prev_cell + i_t*nc_t\n",
        "\n",
        "    #hidden\n",
        "    h_t = o_t*torch.tanh(c_t)\n",
        "\n",
        "    return h_t, c_t\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_norm(prev_x, new_x):\n",
        "  x = torch.cat(prev_x, new_x, dim=0)\n",
        "  x = nn.LayerNorm(x)\n",
        "  return x"
      ],
      "metadata": {
        "id": "dHlOA5i1SlyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuQ3OUL-hvFx"
      },
      "outputs": [],
      "source": [
        "class Transformer_Node(nn.Module):\n",
        "  def __init__(self, hidden_dim, embed_dim, ff_dim, decoder=False):\n",
        "    super(Transformer_Node, self).__init__()\n",
        "\n",
        "    ##layers##\n",
        "    #attention\n",
        "    self.attention = Attention_Node(hidden_dim)\n",
        "    self.decoder = decoder\n",
        "    if(decoder):\n",
        "      self.decoder_encoder_att = Attention_Node(hidden_dim)\n",
        "\n",
        "    #feed forwards\n",
        "    self.linear1 = nn.Linear(hidden_dim, ff_dim)\n",
        "    self.linear2 = nn.Linear(ff_dim, hidden_dim)\n",
        "\n",
        "\n",
        "  #how its connected to the next transformer node\n",
        "  def forward(self, inputs, encoder_hidden=None):\n",
        "    #multihead attention (with mask option) ?\n",
        "    if(self.decoder): # if decoder, only look at values before\n",
        "      attention_out = self.attention.forward(inputs, inputs, masked=True)\n",
        "    else:\n",
        "      attention_out = self.attention.forward(inputs, inputs)\n",
        "\n",
        "    #add and norm\n",
        "    x = add_norm(inputs, attention_out)\n",
        "\n",
        "    #encoder decoder attention\n",
        "    if(self.decoder):\n",
        "      x_prev = x\n",
        "      self.decoder_encoder_att.forward(encoder_hidden, x)\n",
        "      x = add_norm(x_prev, x)\n",
        "\n",
        "    x_prev = x\n",
        "    #feed forward\n",
        "    x = self.linear1(x)\n",
        "    x = self.linear2(x)\n",
        "\n",
        "    #activation function\n",
        "    x = nn.relu(x)\n",
        "\n",
        "    #add and norm\n",
        "    x = add_norm(x_prev, x)\n",
        "\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZYVWsZ8haMS"
      },
      "outputs": [],
      "source": [
        "# class Transformer_Encoder(nn.Module):\n",
        "#   def __init__(self, hidden_dim, n_layers, vocab_size):\n",
        "#     super(Transformer_Encoder, self).__init__()\n",
        "\n",
        "#     #embedding\n",
        "#     self.embedding_node = Embedding_Node(vocab_size, hidden_dim)\n",
        "\n",
        "#     #layers\n",
        "#     self.layers = nn.ModuleList([Transformer_Node(hidden_dim) for i in range(0,n_layers)])\n",
        "\n",
        "#   def forward(self, inputs):\n",
        "#     pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67QpPRLRXRlE"
      },
      "outputs": [],
      "source": [
        "# class Transformer_Node(nn.Module):\n",
        "#   def __init__(self, hidden_dim):\n",
        "#     super(Transformer_Node, self).__init__()\n",
        "\n",
        "#   def forward_encoder(self, inputs):\n",
        "#     pass\n",
        "\n",
        "#   def forward_decoder(self, inputs):\n",
        "#     pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZubrtWaX4r7z"
      },
      "outputs": [],
      "source": [
        "#for this, we basically said that what should be in the __init__ function should be any parameters that could be learned\n",
        "\n",
        "#ok, so for node elements, we'll say that in the __init__ funciton we should have the parameters whos weight should be learned\n",
        "#but for nn layers, well have things that define the layers/ hyperparameters\n",
        "\n",
        "#__init__ layer = anything that needs to be consistently referenced in forward pass (especially parameters that need to be updated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUrdwxzVmO4E"
      },
      "outputs": [],
      "source": [
        "class EncoderNN(nn.Module):\n",
        "  def __init__(self, node_type, n_layers, hidden_dim, embed_dim, vocab_size):\n",
        "    super(EncoderNN, self).__init__()\n",
        "\n",
        "    #model parameters\n",
        "    self.n_layers = n_layers\n",
        "    self.node_type = node_type\n",
        "\n",
        "    #define embedding node\n",
        "    self.embedding_node = Embedding_Node(vocab_size, embed_dim) # I guess this is one node?\n",
        "\n",
        "    #create layer of nodes based on type\n",
        "    if(node_type==\"rnn\"):\n",
        "      self.nodes = nn.ModuleList([RNN_Node(hidden_dim, embed_dim) for i in range(0,n_layers)])\n",
        "    elif(self.node_type=='lstm'):\n",
        "      self.nodes = nn.ModuleList([LSTM_Node(hidden_dim, embed_dim) for i in range(0,n_layers)])\n",
        "\n",
        "    # #store initial hidden states\n",
        "    # self.hidden = torch.randn(n_layers, hidden_dim)\n",
        "    # self.cell = torch.randn(n_layers, hidden_dim) #only really applicable for lstm\n",
        "\n",
        "  #define initial hidden states at run time\n",
        "  def init_hidden(self, batch_size):\n",
        "    init_hidden_node = [torch.zeros(batch_size, hidden_dim) for _ in range(self.n_layers)]\n",
        "    init_cell_node = None\n",
        "    if(self.node_type==\"lstm\"):\n",
        "      init_cell_node = [torch.zeros(batch_size, hidden_dim) for _ in range(self.n_layers)]\n",
        "    return init_hidden_node, init_cell_node\n",
        "\n",
        "\n",
        "  #forward pass - considered\n",
        "  def forward(self, inputs):\n",
        "\n",
        "    #get blank nodes\n",
        "    node_list = self.nodes\n",
        "    print('\\nNode List')\n",
        "    display(node_list)\n",
        "\n",
        "    # #take blank hidden states that define those nodes\n",
        "    # hidden_list = self.hidden\n",
        "    # cell_list = self.cell\n",
        "\n",
        "    #get initial states\n",
        "    hidden_list, cell_list = self.init_hidden(inputs.shape[0])\n",
        "\n",
        "    #store final layer hidden nodes (for attention)\n",
        "    final_hidden_layer = []\n",
        "\n",
        "    print('\\nInitial Input:')\n",
        "    display(inputs)\n",
        "    display(inputs.shape)\n",
        "\n",
        "    #turn input into embedding\n",
        "    embed_input = self.embedding_node.forward(inputs)\n",
        "    print('\\nEmbedding:')\n",
        "    # display(embed_input)\n",
        "    display(embed_input.shape)\n",
        "\n",
        "    #iterate over each word in sentence (# sent=batch size)\n",
        "    for i in range(0, embed_input.shape[1]):\n",
        "      input = embed_input[:,i] #input = (batch_size, hidden_dim) ##1 for each word\n",
        "      print('\\n\\nStart of LSTM:')\n",
        "      print('\\nInitial Input:')\n",
        "      display(input)\n",
        "      display(input.shape)\n",
        "\n",
        "      for i in range(0,self.n_layers):\n",
        "        #if not first word, then previous hidden layer\n",
        "        if i!=0:\n",
        "          input = hidden_list[i-1]\n",
        "\n",
        "          print('\\nNew Input:')\n",
        "          display(input)\n",
        "          print('input shape: {}'.format(input.shape))\n",
        "\n",
        "        print('\\nHidden List at start of {} layer:'.format(i+1))\n",
        "        display(hidden_list)\n",
        "        display(len(hidden_list))\n",
        "\n",
        "        print('\\nHidden List[i]')\n",
        "        display(hidden_list[i])\n",
        "        display(hidden_list[i].shape)\n",
        "\n",
        "        if(cell_list != None):\n",
        "          print('\\n\\nCell List:')\n",
        "          display(cell_list)\n",
        "          display(len(cell_list))\n",
        "\n",
        "          print('\\nCell List[i]')\n",
        "          display(cell_list[i])\n",
        "          display(cell_list[i].shape)\n",
        "\n",
        "        #forward nodes - hidden_list[i] shape = (batch, hidden dim)\n",
        "        if(self.node_type==\"rnn\"):\n",
        "          hidden_list[i] = node_list[i].forward(input, hidden_list[i]) # (batch, hidden dim)\n",
        "        elif(self.node_type=='lstm'):\n",
        "          hidden_list[i], cell_list[i] = node_list[i].forward(input, hidden_list[i], cell_list[i]) # both of size (batch, hidden dim)\n",
        "\n",
        "\n",
        "        #store final hidden layer\n",
        "        final_hidden_layer.append(hidden_list[-1])\n",
        "\n",
        "      break\n",
        "\n",
        "      outputs = hidden_list if self.node_type==\"rnn\" else (hidden_list, cell_list) if self.node_type=='lstm' else None\n",
        "\n",
        "    # for sentence in inputs: #word = (1, # of words w/ pad)\n",
        "    #   print('\\nSentence:')\n",
        "    #   display(sentence)\n",
        "\n",
        "    #   #input is initial embedding\n",
        "    #   input = self.embedding_node.forward(sentence)  #input=(length of word, hidden shape)\n",
        "\n",
        "    #   print('\\nembedding')\n",
        "    #   display(input)\n",
        "    #   display(input.shape)\n",
        "    #   # break\n",
        "\n",
        "    #   for i in range(0,self.n_layers):\n",
        "    #     #if not first word, then previous output\n",
        "    #     if i!=0:\n",
        "    #       input = hidden_list[i-1]\n",
        "\n",
        "    #     print('\\nHidden List:')\n",
        "    #     display(hidden_list)\n",
        "    #     display(hidden_list.shape)\n",
        "\n",
        "    #     print('\\nHidden List[i]')\n",
        "    #     display(hidden_list[i])\n",
        "    #     display(hidden_list[i].shape)\n",
        "\n",
        "    #     #forward nodes\n",
        "    #     if(self.node_type==\"rnn\"):\n",
        "    #       hidden_list[i] = node_list[i].forward(input, hidden_list[i]) # ()\n",
        "    #     elif(self.node_type=='lstm'):\n",
        "    #       hidden_list[i], cell_list[i] = node_list[i].forward(input, hidden_list[i], cell_list[i])\n",
        "\n",
        "    #     print('\\nHidden RNN:')\n",
        "    #     display(hidden_list[i])\n",
        "    #     display(hidden_list[i].shape)\n",
        "    #     break\n",
        "\n",
        "    #   #store final hidden layer\n",
        "    #   final_hidden_layer.append(hidden_list[-1])\n",
        "\n",
        "\n",
        "    # outputs = hidden_list if self.node_type==\"rnn\" else (hidden_list, cell_list) if self.node_type=='lstm' else None\n",
        "\n",
        "    return outputs, final_hidden_layer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "node_type = \"lstm\"\n",
        "n_layers = 2\n",
        "hidden_dim = 4\n",
        "embed_dim = 7\n",
        "#vocab size\n",
        "eng_vocab_size = len(data_dict[\"English Vocabulary\"])\n",
        "it_vocab_size = len(data_dict[\"Italian Vocabulary\"])\n",
        "\n",
        "#create encoder instance\n",
        "encoder = EncoderNN(node_type, n_layers, hidden_dim, embed_dim, eng_vocab_size)\n",
        "\n",
        "#forward step\n",
        "hidden_list, final_hidden_layer = encoder.forward(input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "x2zaz2EZ1hX6",
        "outputId": "56cd4d42-ee08-437c-e73a-832ca0dff4cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Node List\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ModuleList(\n",
              "  (0-1): 2 x LSTM_Node()\n",
              ")"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Initial Input:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tensor([[ 1,  8,  3,  7,  4,  2],\n",
              "        [ 1,  5,  9,  2,  0,  0],\n",
              "        [ 1,  6,  3, 10,  2,  0]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "torch.Size([3, 6])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Embedding:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "torch.Size([3, 6, 7])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Start of LSTM:\n",
            "\n",
            "Initial Input:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tensor([[-0.2171, -0.1733, -0.4649,  0.2809,  0.4340,  0.2792, -0.2099],\n",
              "        [-0.2171, -0.1733, -0.4649,  0.2809,  0.4340,  0.2792, -0.2099],\n",
              "        [-0.2171, -0.1733, -0.4649,  0.2809,  0.4340,  0.2792, -0.2099]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "torch.Size([3, 7])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Hidden List at start of 1 layer:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[tensor([[0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0.]]),\n",
              " tensor([[0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0.]])]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Hidden List[i]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "torch.Size([3, 4])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Cell List:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[tensor([[0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0.]]),\n",
              " tensor([[0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0.]])]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cell List[i]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "torch.Size([3, 4])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "New Input:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tensor([[-0.1025,  0.0467, -0.0173,  0.3402],\n",
              "        [-0.1025,  0.0467, -0.0173,  0.3402],\n",
              "        [-0.1025,  0.0467, -0.0173,  0.3402]], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input shape: torch.Size([3, 4])\n",
            "\n",
            "Hidden List at start of 2 layer:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[tensor([[-0.1025,  0.0467, -0.0173,  0.3402],\n",
              "         [-0.1025,  0.0467, -0.0173,  0.3402],\n",
              "         [-0.1025,  0.0467, -0.0173,  0.3402]], grad_fn=<MulBackward0>),\n",
              " tensor([[0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0.]])]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Hidden List[i]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "torch.Size([3, 4])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Cell List:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[tensor([[-0.2350,  0.0579, -0.0251,  0.7322],\n",
              "         [-0.2350,  0.0579, -0.0251,  0.7322],\n",
              "         [-0.2350,  0.0579, -0.0251,  0.7322]], grad_fn=<AddBackward0>),\n",
              " tensor([[0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0.]])]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cell List[i]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "torch.Size([3, 4])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (4x7 and 4x3)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-0606103671c2>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#forward step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mhidden_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_hidden_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-73-189d629bf187>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     95\u001b[0m           \u001b[0mhidden_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (batch, hidden dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32melif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_type\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'lstm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m           \u001b[0mhidden_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# both of size (batch, hidden dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-70-0051cfaaa31b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, prev_hidden, prev_cell)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m#forget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     f_t = torch.sigmoid(torch.matmul(self.W_f, prev_hidden.T).T+\n\u001b[0;32m---> 43\u001b[0;31m                                      \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                                      b_f)\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m#input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (4x7 and 4x3)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input"
      ],
      "metadata": {
        "id": "M16k_wSP3IDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #forward step\n",
        "# hidden_list, final_hidden_layer = encoder.forward(input)"
      ],
      "metadata": {
        "id": "irGVEb6N2w1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKiXWgCvfAuD"
      },
      "outputs": [],
      "source": [
        "class DecoderNN(nn.Module):\n",
        "  def __init__(self, node_type, n_layers, hidden_dim, vocab_size, attention=False, max_length=30):\n",
        "    super(DecoderNN, self).__init__()\n",
        "\n",
        "    #model parameters\n",
        "    self.n_layers = n_layers\n",
        "    self.node_type = node_type\n",
        "    self.max_length = max_length\n",
        "\n",
        "    #define embedding node\n",
        "    self.embedding_node = Embedding_Node(vocab_size, hidden_dim) # I guess this is one node?\n",
        "\n",
        "    #create layer of nodes based on type\n",
        "    if(node_type==\"rnn\"):\n",
        "      self.nodes = nn.ModuleList([RNN_Node(hidden_dim) for i in range(0,n_layers)])\n",
        "    else:\n",
        "      self.nodes = nn.ModuleList([LSTM_Node(hidden_dim) for i in range(0,n_layers)])\n",
        "\n",
        "    # #store initial hidden states\n",
        "    # #self.hidden = torch.randn(n_layers, hidden_dim)\n",
        "    # self.cell = torch.randn(n_layers, hidden_dim) #only really applicable for lstm\n",
        "    self.prev_hidden = torch.randn(n_layers, hidden_dim)\n",
        "    self.prev_cell = torch.randn(n_layers, hidden_dim) #only really applicable for lstm\n",
        "\n",
        "\n",
        "    #output weights\n",
        "    self.W_out = nn.Parameter(torch.randn(vocab_size, hidden_dim))\n",
        "\n",
        "    #attention\n",
        "    self.attention = attention\n",
        "    if(attention):\n",
        "      self.attention_node = Attention_Node(hidden_dim)\n",
        "\n",
        "  #forward pass - must iterate over\n",
        "  def forward(self, input, prev_hidden, prev_cell=None, encoder_flayer=None):\n",
        "    #start with blank hidden and cell states\n",
        "    hidden_list = self.prev_hidden\n",
        "    cell_list = self.prev_cell\n",
        "\n",
        "    #depending on the length, replace hidden and cell states\n",
        "    hidden_list[:len(prev_hidden)] = prev_hidden\n",
        "    if(prev_cell!=None):\n",
        "      cell_list[:len(prev_cell)] = prev_cell\n",
        "\n",
        "    #initial nodes\n",
        "    node_list = self.nodes\n",
        "\n",
        "    #get embedding of input\n",
        "    input = self.embedding_node.forward(input)\n",
        "\n",
        "\n",
        "    #put input through all layers\n",
        "    for i in range(0,self.n_layers):\n",
        "      if(self.node_type=='rnn'):\n",
        "        hidden_list[i] = node_list[i].forward(input, hidden_list[i])\n",
        "      elif (self.node_type=='lstm'):\n",
        "        hidden_list[i], cell_list[i] = node_list[i].forward(input, hidden_list[i], cell_list[i])\n",
        "\n",
        "      #new input is prev output\n",
        "      input = hidden_list[i]\n",
        "\n",
        "    #attention\n",
        "    if(self.attention):\n",
        "      attention_out = self.attention_node.forward(encoder_flayer, hidden_list[-1])\n",
        "      hidden_list[-1] = torch.cat((hidden_list[-1], attention_out), dim=0)\n",
        "\n",
        "    #generate final output\n",
        "    output = torch.dot(self.W_out, hidden_list[-1])\n",
        "\n",
        "    return output, hidden_list, cell_list\n",
        "\n",
        "  #to generate predictions - could either do this step by step, or generate all predictions in one go\n",
        "  def pred(self, hidden_list, cell_list=None, encoder_flayer=None):\n",
        "    output_list = []\n",
        "    input = 0 # <sos> = start of sentence\n",
        "\n",
        "    #keep predicting unless reach max_length or break out clause\n",
        "    for count in range(0,self.max_length):\n",
        "      input, hidden_list, cell_list = self.forward(input, hidden_list, cell_list)\n",
        "\n",
        "      output_list.append(input)\n",
        "\n",
        "      #<eos>\n",
        "      if(input==1):\n",
        "        break\n",
        "\n",
        "    # #turn into tensor\n",
        "    # output_list = torch.cat(output_list, dim=0)\n",
        "    # display(output_list)\n",
        "\n",
        "    #attention\n",
        "    if(self.attention):\n",
        "      attention_out = self.attention_node.forward(encoder_flayer, output_list)\n",
        "      output_list = torch.cat((output_list, attention_out), dim=0)\n",
        "    else: #DOUBLE CHECK ON THIS\n",
        "      output_list = torch.cat(output_list, dim=0)\n",
        "\n",
        "    #put through softmax\n",
        "    output_list = F.softmax(output_list, dim=0)\n",
        "\n",
        "    return output_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQS3dWtMfNjg"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder_args, decoder_args):\n",
        "    super(Seq2Seq, self).__init__()\n",
        "\n",
        "    #model structure\n",
        "    self.encoder = EncoderNN(*encoder_args)\n",
        "    self.decoder = DecoderNN(*decoder_args)\n",
        "\n",
        "    #set other model params to be ref later\n",
        "    self.encoder.node_type = encoder_args[0]\n",
        "    self.decoder.node_type = decoder_args[0]\n",
        "\n",
        "\n",
        "  #for the full system\n",
        "  def forward(self, inputs, outputs):\n",
        "\n",
        "    #encoder\n",
        "    if (self.encoder.node_type == \"rnn\"):\n",
        "      hidden_list, final_hidden_layer = self.encoder.forward(inputs)\n",
        "      cell_list = None\n",
        "    elif (self.encoder.node_type == \"lstm\"):\n",
        "      hidden_list, cell_list, final_hidden_layer = self.encoder.forward(inputs)\n",
        "\n",
        "    #decoder\n",
        "    pred_out = []\n",
        "    for i in range(0, len(outputs)):\n",
        "      if(self.decoder.node_type == \"rnn\"):\n",
        "        pred_out[i], hidden_list, cell_list = self.decoder.forward(input=outputs[i], prev_hidden=hidden_list, encoder_flayer=final_hidden_layer)\n",
        "      elif (self.decoder.node_type == \"lstm\")\n",
        "        pred_out[i], hidden_list, cell_list = self.decoder.forward(input=outputs[i], prev_hidden=hidden_list, prev_cell=cell_list, encoder_flayer=final_hidden_layer)\n",
        "\n",
        "\n",
        "    return pred_out #must make sure has shape (batch size, seq length, vocab size)\n",
        "\n",
        "\n",
        "  def pred(self, inputs):\n",
        "    # #encoder\n",
        "    if (self.encoder.node_type == \"rnn\"):\n",
        "      hidden_list, final_hidden_layer = self.encoder.forward(inputs)\n",
        "      cell_list = None\n",
        "    elif (self.encoder.node_type == \"lstm\"):\n",
        "      hidden_list, cell_list, final_hidden_layer = self.encoder.forward(inputs)\n",
        "\n",
        "    #decoder\n",
        "    if (self.decoder.node_type == \"rnn\"):\n",
        "      pred_out = self.decoder.pred(hidden_list, encoder_flayer=final_hidden_layer)\n",
        "    elif (self.decoder.node_type == \"lstm\"):\n",
        "      pred_out = self.decoder.pred(hidden_list, cell_list, encoder_flayer=final_hidden_layer)\n",
        "\n",
        "    return pred_out\n",
        "\n",
        "    #input to decoder depends on what the encoder was\n",
        "\n",
        "    # #decoder\n",
        "    # if(self.decoder.node_type == \"rnn\"):\n",
        "    #   outputs = self.decoder.forward(hidden_list)\n",
        "    # else:\n",
        "    #   outputs = self.decoder.forward(hidden_list, cell_list)\n",
        "\n",
        "    #attention\n",
        "\n",
        "\n",
        "    # return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eLejyzHk-V7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAVdhuINsyD3"
      },
      "source": [
        "Notes:\n",
        "\n",
        "- before running - write in comments how each dimension is looking like after each computation - helpful for debugging\n",
        "\n",
        "- incorporate batches!\n",
        "\n",
        "- transformers\n",
        "\n",
        "- beam search\n",
        "\n",
        "- bidirectional\n",
        "\n",
        "- different attention methods\n",
        "\n",
        "-- evaluate with Bleu scores"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run time notes:\n",
        "\n",
        "- embedding size: Common values for the embedding dimension in NLP tasks range from 50 to 300, but larger values like 512 or 1024 can be used for more complex tasks or larger datasets."
      ],
      "metadata": {
        "id": "6X8KzhGEsRrD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GsAm8IVIdpr0"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrt1g5rWdNJruoNHHRJoTr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}