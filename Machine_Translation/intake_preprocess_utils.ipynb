{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smargetic/Natural_Language_Processing/blob/main/Machine_Translation/intake_preprocess_utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYnRsR2coA5g"
      },
      "outputs": [],
      "source": [
        "#runtime data storage\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#show all rows and columns\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "#visualize\n",
        "from IPython.display import display\n",
        "\n",
        "#pytorch\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "#tensorflow\n",
        "import tensorflow as tf\n",
        "#from tensorflow.keras.preprocessing.sequence import pad_sequence as tf_pad_sequence\n",
        "\n",
        "#word/sentence processing\n",
        "import re\n",
        "\n",
        "#data storage\n",
        "import pickle\n",
        "import csv\n",
        "from google.colab import files\n",
        "import os\n",
        "import shutil\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxaLBZTTF8YP"
      },
      "outputs": [],
      "source": [
        "#import files\n",
        "def import_file(fileName, sep=None):\n",
        "  df = pd.read_csv(fileName, sep=sep, on_bad_lines='warn').T.reset_index().T.reset_index(drop=True)\n",
        "  return df\n",
        "\n",
        "#display file with name\n",
        "def get_file_and_disp(fileName, sep=None, stringName=\"\"):\n",
        "  print(\"\\n\" + stringName + \":\")\n",
        "\n",
        "  #if zip file, unzip\n",
        "  if(\".zip\" in fileName):\n",
        "    with zipfile.ZipFile(fileName, 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"./\")\n",
        "\n",
        "    #rename for import\n",
        "    fileName = fileName[:-4]\n",
        "\n",
        "  df = import_file(fileName, sep=sep)\n",
        "  display(df.head())\n",
        "  return df\n",
        "\n",
        "# #english to italian translation\n",
        "# df_eng_it = get_file_and_disp(fileName=\"Sentence pairs in English-Italian - 2024-07-26.tsv\" ,sep='\\t', stringName=\"English to Italian\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jzU5MkCEu1m"
      },
      "outputs": [],
      "source": [
        "#get sample of all data - to be used for experimentation\n",
        "def get_sample(df, frac=.2):\n",
        "  df_sample = df.sample(frac=frac, random_state=42)\n",
        "  df_sample = df_sample.reset_index(drop=True)\n",
        "\n",
        "  return df_sample\n",
        "\n",
        "#store sample\n",
        "def dump_sample(df, fileName):\n",
        "  df.to_csv(fileName, sep='\\t', index=False)\n",
        "\n",
        "# #get sample\n",
        "# df_eng_it = get_sample(df_eng_it, .01)\n",
        "# dump_sample(df_eng_it, \"sentence_pairs_eng_it_01.tsv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UE0x1EJ3n4YS"
      },
      "outputs": [],
      "source": [
        "#double check that there are no null versions for these sentences\n",
        "def count_remove_null(df, index=1, name=\"\"):\n",
        "  #count nulls\n",
        "  nulls = df[index].isnull().sum()+ df[index].eq(\"\").sum()\n",
        "  print(\"Nulls in \" + name + \": {}\".format(nulls))\n",
        "\n",
        "  #remove nulls\n",
        "  if(nulls>0):\n",
        "    df.dropna(subset=[index], inplace=True)\n",
        "    df = df[df[index]!=\"\"]\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "# df_eng_it = count_remove_null(df_eng_it, index=1, name=\"English\")\n",
        "# df_eng_it = count_remove_null(df_eng_it, index=3, name=\"Italian\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCx3YAL5LXoE"
      },
      "outputs": [],
      "source": [
        "# #rename columns\n",
        "# df_eng_it.columns = ['eng_id', 'eng_sentence', 'it_id', 'it_sentence']\n",
        "# df_eng_it.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmC_HExwP6tm"
      },
      "outputs": [],
      "source": [
        "#seperates 1-1 translations\n",
        "def one_to_one_translations(df):\n",
        "  df_sing = df.copy().loc[~df.duplicated(subset='eng_sentence', keep=False), :]\n",
        "  df_sing = df_sing.loc[~df_sing.duplicated(subset='it_sentence',keep=False), :]\n",
        "  df_sing.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  print('\\nOrig size: {}'.format(df.shape[0]))\n",
        "  print('Singular size: {}'.format(df_sing.shape[0]))\n",
        "\n",
        "  return df_sing\n",
        "\n",
        "# df_eng_it_sing = one_to_one_translations(df_eng_it)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DH1yTaor7dxG"
      },
      "outputs": [],
      "source": [
        "##tokenize\n",
        "\n",
        "#get tokens\n",
        "def tokenize(sentence):\n",
        "  sent = re.findall(r'\\b\\w+\\b|[^\\w\\s]', sentence)\n",
        "  #add start of sentence and eos tokens\n",
        "  sent.insert(0, '<sos>')\n",
        "  sent.append('<eos>')\n",
        "\n",
        "  return sent\n",
        "\n",
        "#get vocabulary as dictionary - with values as indecies\n",
        "def get_vocab(df, col):\n",
        "  unique_tokens = list(np.unique(np.hstack(np.array(df[col]))))\n",
        "\n",
        "  #move start of sentence and end of sentence tokens to front\n",
        "  unique_tokens.insert(0, unique_tokens.pop(unique_tokens.index(\"<eos>\")))\n",
        "  unique_tokens.insert(0, unique_tokens.pop(unique_tokens.index(\"<sos>\")))\n",
        "\n",
        "  vocab = {k: v+2 for v, k in enumerate(unique_tokens)}\n",
        "\n",
        "  return vocab\n",
        "\n",
        "#encode for vocab\n",
        "def encode_vocab(tokens, vocab):\n",
        "    return [vocab[token] for token in tokens]\n",
        "\n",
        "#full tokenization - returns modified pandas df, and vocabs\n",
        "def tokenize_full(df, name=\"\"):\n",
        "  print('\\n'+name+\":\")\n",
        "\n",
        "  #split words into tokens\n",
        "  df['eng_tokens'] = df['eng_sentence'].apply(tokenize)\n",
        "  df['it_tokens'] = df['it_sentence'].apply(tokenize)\n",
        "\n",
        "  #get vocab\n",
        "  eng_vocab = get_vocab(df, 'eng_tokens')\n",
        "  it_vocab = get_vocab(df, 'it_tokens')\n",
        "\n",
        "  print(\"\\tEnglish vocabulary size: {}\".format(len(eng_vocab)))\n",
        "  print(\"\\tItalian vocabulary size: {}\".format(len(it_vocab)))\n",
        "\n",
        "  #encode for vocab\n",
        "  df['eng_tokens_enc'] = df['eng_tokens'].apply(lambda x: encode_vocab(x, eng_vocab))\n",
        "  df['it_tokens_enc'] = df['it_tokens'].apply(lambda x: encode_vocab(x, it_vocab))\n",
        "\n",
        "  return df, eng_vocab, it_vocab\n",
        "\n",
        "# #tokenize\n",
        "# df_eng_it, eng_vocab, it_vocab = tokenize_full(df_eng_it, name=\"English/Italian Translations\")\n",
        "# df_eng_it_sing, eng_vocab_sing, it_vocab_sing = tokenize_full(df_eng_it_sing, name=\"English/Italian One-One Translations\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0jW4VlN3twd"
      },
      "outputs": [],
      "source": [
        "### pytorch ###\n",
        "\n",
        "#turn column of lists to column of pytorch tensors\n",
        "def turn_list_col_pytorch(df, col):\n",
        "  new_col = col + '_p'\n",
        "  df[new_col] = df[col].apply(lambda x: torch.tensor(x))\n",
        "  return df\n",
        "\n",
        "#add padding\n",
        "def pytorch_pad(df, col):\n",
        "  vals = pad_sequence(df[col], batch_first=True, padding_value=0)\n",
        "  df[col] = list(vals)\n",
        "\n",
        "  return df, vals\n",
        "\n",
        "#seperate data into batches\n",
        "def pytorch_batch(eng_tens, it_tens, batch_size=64):\n",
        "  dataset = TensorDataset(eng_tens, it_tens)\n",
        "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  return dataset, dataloader\n",
        "\n",
        "#all processing to turn list columns to pytorch objects\n",
        "def pytorch_preproc(df):\n",
        "  #turn column of lists to column of pytorch tensors\n",
        "  df = turn_list_col_pytorch(df, 'eng_tokens_enc')\n",
        "  df = turn_list_col_pytorch(df, 'it_tokens_enc')\n",
        "\n",
        "  #add padding\n",
        "  df, eng_tens = pytorch_pad(df, 'eng_tokens_enc_p')\n",
        "  df, it_tens = pytorch_pad(df, 'it_tokens_enc_p')\n",
        "\n",
        "  #get batches\n",
        "  dataset, dataloader = pytorch_batch(eng_tens, it_tens)\n",
        "\n",
        "  # return df, eng_tens, it_tens, dataloader\n",
        "  return df, dataloader\n",
        "\n",
        "\n",
        "# # df_eng_it, eng_tens, it_tens, dataloader = pytorch_preproc(df_eng_it)\n",
        "# df_eng_it, dataloader_p = pytorch_preproc(df_eng_it)\n",
        "# df_eng_it.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXqZmagQEbCd"
      },
      "outputs": [],
      "source": [
        "### tensorflow ###\n",
        "\n",
        "#add padding\n",
        "def tensorflow_pad(df, col):\n",
        "  new_col = col + '_t'\n",
        "  df[new_col] = list(tf.keras.preprocessing.sequence.pad_sequences(df[col].tolist(), padding='post'))\n",
        "  return df\n",
        "\n",
        "#turn list column to list of tensorflow objects\n",
        "def turn_list_col_tensorflow(df, col1, col2):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((df[col1].to_list(), df[col2].to_list()))\n",
        "  return dataset\n",
        "\n",
        "#padding and batch\n",
        "def tensorflow_pad_batch(dataset, batch_size=64):\n",
        "  padded_shapes = ([None], [None])\n",
        "  padding_values = (tf.constant(0, dtype=tf.int32), tf.constant(0, dtype=tf.int32))\n",
        "\n",
        "  dataset = dataset.padded_batch(batch_size, padded_shapes=padded_shapes, padding_values=padding_values)\n",
        "\n",
        "  return dataset\n",
        "\n",
        "#all processing to turn list columns to tensorflow objects\n",
        "def tensorflow_preproc(df):\n",
        "  #turn column of lists to column of tensorflow tensors\n",
        "  df = tensorflow_pad(df, 'eng_tokens_enc')\n",
        "  df = tensorflow_pad(df, 'it_tokens_enc')\n",
        "\n",
        "  #turn list column to list of tensorflow objects\n",
        "  dataset = turn_list_col_tensorflow(df, 'eng_tokens_enc_t', 'it_tokens_enc_t')\n",
        "\n",
        "  #add padding and batch\n",
        "  dataset = tensorflow_pad_batch(dataset)\n",
        "\n",
        "  return df, dataset\n",
        "\n",
        "# df_eng_it, dataloader_t = tensorflow_preproc(df_eng_it)\n",
        "# df_eng_it.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YSNkxx5N--9"
      },
      "outputs": [],
      "source": [
        "# # Example usage\n",
        "# for input_batch, target_batch in dataset:\n",
        "#     print(\"Input batch:\", input_batch)\n",
        "#     print(\"Target batch:\", target_batch)\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3BFzMCE4aY2"
      },
      "outputs": [],
      "source": [
        "#comprehensive function to get all data\n",
        "def data_preprocessing(df_eng_it, pytorchB=True, tensorflowB=True, store=True, sample=False, download=True, sing=True):\n",
        "  # #english to italian translation\n",
        "  # df_eng_it = get_file_and_disp(fileName=\"Sentence pairs in English-Italian - 2024-07-26.tsv\" ,sep='\\t', stringName=\"English to Italian\")\n",
        "\n",
        "\n",
        "  #remove nulls\n",
        "  df_eng_it = count_remove_null(df_eng_it, index=1, name=\"English\")\n",
        "  df_eng_it = count_remove_null(df_eng_it, index=3, name=\"Italian\")\n",
        "\n",
        "  #rename columns\n",
        "  df_eng_it.columns = ['eng_id', 'eng_sentence', 'it_id', 'it_sentence']\n",
        "\n",
        "  #get 1-1 translations\n",
        "  df_eng_it_sing = None\n",
        "  if(sing):\n",
        "    df_eng_it_sing = one_to_one_translations(df_eng_it)\n",
        "\n",
        "  #tokenize\n",
        "  df_eng_it, eng_vocab, it_vocab = tokenize_full(df_eng_it, name=\"English/Italian Translations\")\n",
        "  eng_vocab_sing, it_vocab_sing = None, None\n",
        "  if(sing):\n",
        "    df_eng_it_sing, eng_vocab_sing, it_vocab_sing = tokenize_full(df_eng_it_sing, name=\"English/Italian One-One Translations\")\n",
        "\n",
        "  #turn into pytorch\n",
        "  dataloader_p, dataloader_p_sing = None, None\n",
        "  if(pytorchB):\n",
        "    df_eng_it, dataloader_p = pytorch_preproc(df_eng_it)\n",
        "    if(sing):\n",
        "      df_eng_it_sing, dataloader_p_sing = pytorch_preproc(df_eng_it_sing)\n",
        "\n",
        "    print('\\nDone with Pytorch.')\n",
        "\n",
        "  #turn into tensorflow\n",
        "  dataloader_t, dataloader_t_sing = None, None\n",
        "  if(tensorflowB):\n",
        "    df_eng_it, dataloader_t = tensorflow_preproc(df_eng_it)\n",
        "    if(sing):\n",
        "      df_eng_it_sing, dataloader_t_sing = tensorflow_preproc(df_eng_it_sing)\n",
        "\n",
        "    print('\\nDone with TensorFlow.')\n",
        "\n",
        "\n",
        "  #for data storage\n",
        "  dataList = [df_eng_it, df_eng_it_sing, dataloader_p, dataloader_t, dataloader_p_sing, dataloader_t_sing,\n",
        "                eng_vocab, it_vocab, eng_vocab_sing, it_vocab_sing]\n",
        "\n",
        "  baseNames = ['English-Italian Dataframe ', 'English-Italian SING Dataframe ',\n",
        "                  'English-Italian Pytorch Dataloader ', 'English-Italian TensorFlow Dataloader ',\n",
        "                  'English-Italian Pytorch SING Dataloader ', 'English-Italian TensorFlow SING Dataloader ',\n",
        "                  'English Vocabulary ', 'Italian Vocabulary ',\n",
        "                  'English Vocabulary SING ', 'Italian Vocabulary SING ']\n",
        "\n",
        "\n",
        "  #stored seperately b/c of ram constraints\n",
        "  if(store):\n",
        "\n",
        "    #incase we're told that this is not a sample\n",
        "    sample = 100 if sample == False else int(sample*100)\n",
        "\n",
        "    #create folder if not existant\n",
        "    folderName = \"English-Italian Data - \" + str(sample) + \"%/\"\n",
        "    if not os.path.exists(folderName):\n",
        "        os.mkdir(folderName)\n",
        "\n",
        "    #add in sample and folder name\n",
        "    fileNames = [folderName + name + '- '+str(sample) +\"%\" for name in baseNames]\n",
        "\n",
        "    #add file type\n",
        "    fileNames = [name+\".csv\" if \"Dataframe\" in name\n",
        "                 else name+'.pth' if \"Pytorch\" in name\n",
        "                 else name+'.pkl' if \" Vocabulary\" in name\n",
        "                 else name for name in fileNames]\n",
        "\n",
        "    #add file type\n",
        "    # fileNames = [name+\".csv\" if \"Dataframe\" in name el name+'.pkl' for name in baseNames]\n",
        "\n",
        "    print('\\n---------------------------------------------------------------------------')\n",
        "    for i in range(0,len(fileNames)):\n",
        "      #store sing only if it was set\n",
        "      if((sing and (\"SING\" in fileNames[i])) or (\"SING\" not in fileNames[i])):\n",
        "        print('\\nSaving '+ fileNames[i] +\"...\")\n",
        "\n",
        "        #csv file\n",
        "        if(\".csv\" in fileNames[i]):\n",
        "          dataList[i].to_csv(fileNames[i], index=False)\n",
        "        #pytorch\n",
        "        elif ('.pth' in fileNames[i]):\n",
        "          if(pytorchB):\n",
        "            torch.save(dataList[i], fileNames[i])\n",
        "        #vocab\n",
        "        elif ('.pkl' in fileNames[i]):\n",
        "          with open(fileNames[i], 'wb') as f:\n",
        "            pickle.dump(dataList[i], f)\n",
        "        #tensorflow\n",
        "        else:\n",
        "          if(tensorflowB):\n",
        "            tf.data.experimental.save(dataList[i], fileNames[i])\n",
        "\n",
        "    #create zip of folder\n",
        "    shutil.make_archive(folderName[:-1], 'zip', folderName)\n",
        "\n",
        "    #download if set\n",
        "    if(download):\n",
        "      print('\\nDownloading ' + folderName[:-1] + \" Directory...\")\n",
        "      files.download(folderName[:-1]+'.zip')\n",
        "\n",
        "    print('Done.')\n",
        "\n",
        "\n",
        "  #dictionary of data values\n",
        "  data_dict = dict(zip(baseNames, dataList))\n",
        "\n",
        "\n",
        "  return data_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#zip and download - for inidividual tsv files\n",
        "def zip_download(fileName):\n",
        "  #create zip of folder\n",
        "  shutil.make_archive(fileName, 'zip', fileName)\n",
        "\n",
        "  #download file\n",
        "  files.download(fileName+\".zip\")"
      ],
      "metadata": {
        "id": "fmLxW00kHUv7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqICkwQzyuqcBPbUZkpUQ8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}