{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkXPPLYocDDo6J4luiL/WK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smargetic/Natural_Language_Processing/blob/main/Machine_Translation/Data_Intake_and_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6-_NHTm2kVcs"
      },
      "outputs": [],
      "source": [
        "#all translations have been obtained at tatoeba.org on July 26th, 2024"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data storage\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#show all rows and columns\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "#visualize\n",
        "from IPython.display import display\n",
        "\n",
        "#pytorch\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "#tensorflow\n",
        "import tensorflow as tf\n",
        "#from tensorflow.keras.preprocessing.sequence import pad_sequence as tf_pad_sequence\n",
        "\n",
        "#word/sentence processing\n",
        "import re\n",
        "\n",
        "#store data\n",
        "import pickle"
      ],
      "metadata": {
        "id": "bYnRsR2coA5g"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import files\n",
        "def import_file(fileName, sep=None):\n",
        "  df = pd.read_csv(fileName, sep=sep, on_bad_lines='warn').T.reset_index().T.reset_index(drop=True)\n",
        "  return df\n",
        "\n",
        "#display file with name\n",
        "def get_file_and_disp(fileName, sep=None, stringName=\"\"):\n",
        "  print(\"\\n\" + stringName + \":\")\n",
        "  df = import_file(fileName, sep=sep)\n",
        "  display(df.head())\n",
        "  return df\n",
        "\n",
        "# #english to italian translation\n",
        "# df_eng_it = get_file_and_disp(fileName=\"Sentence pairs in English-Italian - 2024-07-26.tsv\" ,sep='\\t', stringName=\"English to Italian\")\n"
      ],
      "metadata": {
        "id": "WxaLBZTTF8YP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#double check that there are no null versions for these sentences\n",
        "def count_remove_null(df, index=1, name=\"\"):\n",
        "  #count nulls\n",
        "  nulls = df[index].isnull().sum()+ df[index].eq(\"\").sum()\n",
        "  print(\"Nulls in \" + name + \": {}\".format(nulls))\n",
        "\n",
        "  #remove nulls\n",
        "  if(nulls>0):\n",
        "    df.dropna(subset=[index], inplace=True)\n",
        "    df = df[df[index]!=\"\"]\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "# df_eng_it = count_remove_null(df_eng_it, index=1, name=\"English\")\n",
        "# df_eng_it = count_remove_null(df_eng_it, index=3, name=\"Italian\")"
      ],
      "metadata": {
        "id": "UE0x1EJ3n4YS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #rename columns\n",
        "# df_eng_it.columns = ['eng_id', 'eng_sentence', 'it_id', 'it_sentence']\n",
        "# df_eng_it.head()"
      ],
      "metadata": {
        "id": "bCx3YAL5LXoE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#seperates 1-1 translations\n",
        "def one_to_one_translations(df):\n",
        "  df_sing = df.copy().loc[~df.duplicated(subset='eng_sentence', keep=False), :]\n",
        "  df_sing = df_sing.loc[~df_sing.duplicated(subset='it_sentence',keep=False), :]\n",
        "  df_sing.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  print('\\nOrig size: {}'.format(df.shape[0]))\n",
        "  print('Singular size: {}'.format(df_sing.shape[0]))\n",
        "\n",
        "  return df_sing\n",
        "\n",
        "# df_eng_it_sing = one_to_one_translations(df_eng_it)"
      ],
      "metadata": {
        "id": "xmC_HExwP6tm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##tokenize\n",
        "\n",
        "#get tokens\n",
        "def tokenize(sentence):\n",
        "    return re.findall(r'\\b\\w+\\b|[^\\w\\s]', sentence)\n",
        "\n",
        "#get vocabulary as dictionary - with values as indecies\n",
        "def get_vocab(df, col):\n",
        "  unique_tokens = np.unique(np.hstack(np.array(df[col])))\n",
        "  vocab = {k: v+1 for v, k in enumerate(unique_tokens)} #0 will be padding\n",
        "  return vocab\n",
        "\n",
        "#encode for vocab\n",
        "def encode_vocab(tokens, vocab):\n",
        "    return [vocab[token] for token in tokens]\n",
        "\n",
        "#full tokenization - returns modified pandas df, and vocabs\n",
        "def tokenize_full(df, name=\"\"):\n",
        "  print('\\n'+name+\":\")\n",
        "\n",
        "  #split words into tokens\n",
        "  df['eng_tokens'] = df['eng_sentence'].apply(tokenize)\n",
        "  df['it_tokens'] = df['it_sentence'].apply(tokenize)\n",
        "\n",
        "  #get vocab\n",
        "  eng_vocab = get_vocab(df, 'eng_tokens')\n",
        "  it_vocab = get_vocab(df, 'it_tokens')\n",
        "\n",
        "  print(\"\\tEnglish vocabulary size: {}\".format(len(eng_vocab)))\n",
        "  print(\"\\tItalian vocabulary size: {}\".format(len(it_vocab)))\n",
        "\n",
        "  #encode for vocab\n",
        "  df['eng_tokens_enc'] = df['eng_tokens'].apply(lambda x: encode_vocab(x, eng_vocab))\n",
        "  df['it_tokens_enc'] = df['it_tokens'].apply(lambda x: encode_vocab(x, it_vocab))\n",
        "\n",
        "  return df, eng_vocab, it_vocab\n",
        "\n",
        "# #tokenize\n",
        "# df_eng_it, eng_vocab, it_vocab = tokenize_full(df_eng_it, name=\"English/Italian Translations\")\n",
        "# df_eng_it_sing, eng_vocab_sing, it_vocab_sing = tokenize_full(df_eng_it_sing, name=\"English/Italian One-One Translations\")\n"
      ],
      "metadata": {
        "id": "DH1yTaor7dxG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### pytorch ###\n",
        "\n",
        "#turn column of lists to column of pytorch tensors\n",
        "def turn_list_col_pytorch(df, col):\n",
        "  new_col = col + '_p'\n",
        "  df[new_col] = df[col].apply(lambda x: torch.tensor(x))\n",
        "  return df\n",
        "\n",
        "#add padding\n",
        "def pytorch_pad(df, col):\n",
        "  vals = pad_sequence(df[col], batch_first=True, padding_value=0)\n",
        "  df[col] = list(vals)\n",
        "\n",
        "  return df, vals\n",
        "\n",
        "#seperate data into batches\n",
        "def pytorch_batch(eng_tens, it_tens, batch_size=64):\n",
        "  dataset = TensorDataset(eng_tens, it_tens)\n",
        "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  return dataloader\n",
        "\n",
        "#all processing to turn list columns to pytorch objects\n",
        "def pytorch_preproc(df):\n",
        "  #turn column of lists to column of pytorch tensors\n",
        "  df = turn_list_col_pytorch(df, 'eng_tokens_enc')\n",
        "  df = turn_list_col_pytorch(df, 'it_tokens_enc')\n",
        "\n",
        "  #add padding\n",
        "  df, eng_tens = pytorch_pad(df, 'eng_tokens_enc_p')\n",
        "  df, it_tens = pytorch_pad(df, 'it_tokens_enc_p')\n",
        "\n",
        "  #get batches\n",
        "  dataloader = pytorch_batch(eng_tens, it_tens)\n",
        "\n",
        "  # return df, eng_tens, it_tens, dataloader\n",
        "  return df, dataloader\n",
        "\n",
        "\n",
        "# # df_eng_it, eng_tens, it_tens, dataloader = pytorch_preproc(df_eng_it)\n",
        "# df_eng_it, dataloader_p = pytorch_preproc(df_eng_it)\n",
        "# df_eng_it.head()"
      ],
      "metadata": {
        "id": "G0jW4VlN3twd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### tensorflow ###\n",
        "\n",
        "#add padding\n",
        "def tensorflow_pad(df, col):\n",
        "  new_col = col + '_t'\n",
        "  df[new_col] = list(tf.keras.preprocessing.sequence.pad_sequences(df[col].tolist(), padding='post'))\n",
        "  return df\n",
        "\n",
        "#turn list column to list of tensorflow objects\n",
        "def turn_list_col_tensorflow(df, col1, col2):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((df[col1], df[col2]))\n",
        "  return dataset\n",
        "\n",
        "#padding and batch\n",
        "def tensorflow_pad_batch(dataset, batch_size=64):\n",
        "  padded_shapes = ([None], [None])\n",
        "  padding_values = (tf.constant(0, dtype=tf.int32), tf.constant(0, dtype=tf.int32))\n",
        "\n",
        "  dataset = dataset.padded_batch(batch_size, padded_shapes=padded_shapes, padding_values=padding_values)\n",
        "\n",
        "  return dataset\n",
        "\n",
        "#all processing to turn list columns to tensorflow objects\n",
        "def tensorflow_preproc(df):\n",
        "  #turn column of lists to column of tensorflow tensors\n",
        "  df = tensorflow_pad(df, 'eng_tokens_enc')\n",
        "  df = tensorflow_pad(df, 'it_tokens_enc')\n",
        "\n",
        "  #turn list column to list of tensorflow objects\n",
        "  dataset = turn_list_col_tensorflow(df, 'eng_tokens_enc_t', 'it_tokens_enc_t')\n",
        "\n",
        "  #add padding and batch\n",
        "  dataset = tensorflow_pad_batch(dataset)\n",
        "\n",
        "  return df, dataset\n",
        "\n",
        "# df_eng_it, dataloader_t = tensorflow_preproc(df_eng_it)\n",
        "# df_eng_it.head()"
      ],
      "metadata": {
        "id": "DXqZmagQEbCd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Example usage\n",
        "# for input_batch, target_batch in dataset:\n",
        "#     print(\"Input batch:\", input_batch)\n",
        "#     print(\"Target batch:\", target_batch)\n",
        "#     break"
      ],
      "metadata": {
        "id": "2YSNkxx5N--9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#comprehensive function to get all data\n",
        "def data_preprocessing(pytorchB=True, tensorflowB=True, store=True):\n",
        "  #english to italian translation\n",
        "  df_eng_it = get_file_and_disp(fileName=\"Sentence pairs in English-Italian - 2024-07-26.tsv\" ,sep='\\t', stringName=\"English to Italian\")\n",
        "\n",
        "  #remove nulls\n",
        "  print('\\n')\n",
        "  df_eng_it = count_remove_null(df_eng_it, index=1, name=\"English\")\n",
        "  df_eng_it = count_remove_null(df_eng_it, index=3, name=\"Italian\")\n",
        "\n",
        "  #rename columns\n",
        "  df_eng_it.columns = ['eng_id', 'eng_sentence', 'it_id', 'it_sentence']\n",
        "\n",
        "  #get 1-1 translations\n",
        "  df_eng_it_sing = one_to_one_translations(df_eng_it)\n",
        "\n",
        "  #tokenize\n",
        "  df_eng_it, eng_vocab, it_vocab = tokenize_full(df_eng_it, name=\"English/Italian Translations\")\n",
        "  df_eng_it_sing, eng_vocab_sing, it_vocab_sing = tokenize_full(df_eng_it_sing, name=\"English/Italian One-One Translations\")\n",
        "\n",
        "  #turn into pytorch\n",
        "  dataloader_p, dataloader_p_sing = None, None\n",
        "  if(pytorchB):\n",
        "    df_eng_it, dataloader_p = pytorch_preproc(df_eng_it)\n",
        "    df_eng_it_sing, dataloader_p_sing = pytorch_preproc(df_eng_it_sing)\n",
        "\n",
        "  #turn into tensorflow\n",
        "  dataloader_t, dataloader_t_sing = None, None\n",
        "  if(tensorflowB):\n",
        "    df_eng_it, dataloader_t = tensorflow_preproc(df_eng_it)\n",
        "    df_eng_it_sing, dataloader_t_sing = tensorflow_preproc(df_eng_it_sing)\n",
        "\n",
        "  #make dictionary of values\n",
        "  data_dict = {\"df_eng_it\": df_eng_it, \"df_eng_it_sing\": df_eng_it_sing,\n",
        "               \"dataloader_p\": dataloader_p, \"dataloader_t\": dataloader_t,\n",
        "               \"dataloader_p_sing\": dataloader_p_sing, \"dataloader_t_sing\": dataloader_t_sing,\n",
        "               \"eng_vocab\": eng_vocab, \"it_vocab\": it_vocab,\n",
        "               \"eng_vocab_sing\": eng_vocab_sing, \"it_vocab_sing\": it_vocab_sing}\n",
        "\n",
        "  #store dictionary values\n",
        "  if(store):\n",
        "    with open('nn_data_dict.pkl', 'wb') as fp:\n",
        "        pickle.dump(data_dict, fp)\n",
        "        print('dictionary saved successfully to file')\n",
        "\n",
        "  return data_dict\n",
        "\n",
        "data_preprocessing()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "i3BFzMCE4aY2",
        "outputId": "630a7c95-35e1-4e51-c5a1-68339576ea19"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "English to Italian:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Sentence pairs in English-Italian - 2024-07-26.tsv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-3d3db318f666>\u001b[0m in \u001b[0;36m<cell line: 48>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mdata_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-3d3db318f666>\u001b[0m in \u001b[0;36mdata_preprocessing\u001b[0;34m(pytorchB, tensorflowB, store)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdata_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpytorchB\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorflowB\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;31m#english to italian translation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mdf_eng_it\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_file_and_disp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Sentence pairs in English-Italian - 2024-07-26.tsv\"\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstringName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"English to Italian\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;31m#remove nulls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-5d4961dc616b>\u001b[0m in \u001b[0;36mget_file_and_disp\u001b[0;34m(fileName, sep, stringName)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_file_and_disp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstringName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstringName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\":\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-5d4961dc616b>\u001b[0m in \u001b[0;36mimport_file\u001b[0;34m(fileName, sep)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#import files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimport_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_bad_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'warn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Sentence pairs in English-Italian - 2024-07-26.tsv'"
          ]
        }
      ]
    }
  ]
}